1페이지
자연어처리 책 정리 발표를 진행하게된 손희락이라고 합니다.

2페이지
4장과 6장에서 나온 내용중 이 순서대로 발표를 진행하겠습니다.

3페이지
1번 서브워드 분절입니다.

4페이지
서브워드 분절이란 하나의 단어(혹은 토큰)는 여러개의 subword 의 조합으로 이루어져 있다는 가정하에 subword 단위의 토크나이징을 수행하여 이해하려는 목적을 가진 전처리 작업을 말합니다.
영어는 라틴어 한국어는 중국어를 기반으로 형성된 언어이기에 많은 단어가 서브워드들로 구성됩니다.
따라서 적절한 서브워드를 발견하여 해당 단위로 쪼개어주면 어휘 수와 희소성을 효과적으로 줄일 수 있습니다. 또한 UNK 토큰에 대한 효율적인 처리로 OOV 문제를 완화할 수 있습니다.

5페이지
대표적인 서브워드 분절 알고리즘으로는 BPE 알고리즘이 있습니다. 이 방법은 데이터 압축을 위한 알고리즘으로 탄생했지만 현재는 NLP 분야의 대표적인 토크나이징 방법으로 활용되고 있습니다.

6페이지
BPE 학습과정은 단어와 빈도를 포함한 사전을 만들고 character 단위로 분절 후 pair 별 빈도를 카운트합니다. 그 중 최빈도 pair 를 골라서 merge 를 수행하고 pair 별 빈도 카운트를 업데이트 합니다. 그리고 다시 3번과 4번과정을 반복합니다. 이 반복은 정해진 반복횟수 만큼 진행합니다.
적용방법은 다음과 같습니다. 각 단어를 character 단위로 분절 후 단어 내에서 학습 과정에서 merge 에 활용된 pair 의 순서대로 merge 를 수행합니다.

7페이지
Training Example 을 보겠습니다. low 5개, lower 2개, newest 6개, widest 3개가 주어진 경우입니다. 각각 단어를 character 단위로 분절 후 end of word token 을 추가해줍니다. 그 후 연속된 character 2개를 pair 로 만들어 count 해줍니다. 그 중 가장 많은 count 를 가진 pair ('e', 's') 를 merge 해줍니다. dictionary 를 merge 한 pair 에 맞게 업데이트 후 다시 pair 의 빈도를 구합니다. 가장 많은 count 를 가진 pair ('es', 't') 를 merge 해줍니다. 이와 같은 방식으로 정해진 횟수만큼 반복을 진행합니다.

8페이지
정해진 반복 횟수를 다 돌고나면 best pair 를 순서대로 모아놓은 모델이 만들어집니다.

9페이지
그러면 학습에 사용되지 않았던 latest news 에 대해서 분절을 적용해보겠습니다. 이 경우 character 단위로 분절 후 end of word token 을 공백에 추가해줍니다. 그 후 이전장에서 학습을 진행하면서 만들어진 best pair 순서를 적용해 서브워드 분절을 적용합니다. es, est, est</w>, ne, new 순으로 적용되어 마무리됩니다.

10페이지
서브워드 분절 사용시 주의할 점이 있습니다. training set 의 통계의 기반해서 segmentation 이 결정되기 때문에 test set 과 같은 도메인인게 중요합니다. 다른 경우 제대로된 분절이 진행되지 않을 수 있습니다. 그리고 한국어의 경우 띄어쓰기가 제멋대로인 경우가 많아서 normalization 없이 바로 subword segmentation 을 적용하는 것은 위험합니다. 이를 위해 형태소 분석기를 통한 tokenization 을 진행한 이후 subword segmentation 을 적용하는 것을 권장합니다.

11페이지
2번 분절 복원입니다.

12페이지
분절 복원이 필요한 이유는 자연어 생성 task 에서 모델이 이해하는 토큰의 형태가 아닌 사람이 읽을 수 있는 형태로 복원하는 과정이 필요하기 때문입니다.

13페이지
분절 복원을 위해 tokenization 을 수행하면서 기존 띄어쓰기와 구분을 위해 '_'처럼 생긴 특수문자를 기존 띄어쓰기 위치에 추가해줍니다. 그 후에 subword segmentation 을 수행하면서 생기는 공백과 구분을 위해 원문 띄어쓰기 부분과 tokenization 띄어쓰기 부분에 '_' 를 다시 추가해줍니다. 그러면 '_' 가 2개인 부분은 원문에서 띄어쓰기 되어있는 부분이고 '_' 가 1개인 부분은 tokenization 수행 과정에서 생긴 띄어쓰기 되어있는 부분이고 subword segmentation 을 진행한 후 생긴 띄어쓰기 부분이 모두 구별됩니다.

14페이지
분절 복원 Detokenization 은 먼저 공백을 제거해줍니다. 띄어쓰기가 없어진 상태에서 '_' 2개를 공백으로 치환해줍니다. 마지막으로 '_' 1개를 제거해줍니다. 이러면 원래대로 문장이 복원됩니다.

15페이지
3번 차원 축소입니다.

16페이지
높은 차원에서 데이터를 표현하면 희소성 문제가 많이 나타나게 됩니다. 그래서 같은 정보를 표현할 때 더 낮은 차원을 사용하는 것이 중요합니다. 이를 위한 방법이 차원 축소 입니다.

17페이지
대표적인 차원 축소 방법은 주성분 분석(PCA) 입니다. 원 데이터의 분포를 최대한 보존하면서 고차원 공간의 데이터를 저차원 공간으로 변환합니다. 기존의 변수를 조합하여 서로 연관성이 없는 새로운 변수, 즉 주성분들을 만들어냅니다. 만약 3개의 변수가 있는 데이터에서 기존의 변수를 조합해 3개의 주성분을 만들 수 있는데 만일 주성분1이 원 데이터의 성질의 약 90%를 보존한다면, 10% 정도의 정보는 잃어버리더라도, 합리적인 분석에 큰 무리가 없으므로 PC1 만 택해서 1차원 데이터로 차원을 줄일 수 있습니다.

18페이지
주의할 점으로는 실제 데이터의 위치와 축소하고자 하는 평면에 투사된 점의 거리가 생길 수 밖에 없고 이건 곧 정보의 손실을 의미합니다. 그리고 너무 많은 정보가 손실되면 효율적으로 정보를 학습하거나 복구하기 어려운데 결국 높은 차원에서 지나치게 낮은 차원으로 축소하기는 어렵습니다. PCA 는 선형적인 방법이라서 데이터가 비선형적으로 구성될수록 잘 되지 않을 수 있습니다.

19페이지
이때 높은 차원에 존재하는 데이터들의 경우, 실제로는 데이터들을 아우르는 낮은 차원의 다양체가 존재한다는 매니폴드 가설을 통해 비선형적인 데이터의 차원 축소에 더 효율적으로 접근할 수 있습니다.

20페이지
예를 들어 3차원 공간에 분포한 데이터를 아우르는 소용돌이 모양의 구부러진 2차원 매니폴드가 존재할 수 있습니다. 이런 매니폴드를 찾을 수 있다면 주성분 분석처럼 데이터를 고차원 평면에 선형적으로 투사하며 생긴 손실을 최소화 할 수 있습니다. 특징은 고차원상에서 가까운 거리에 있던 데이터 포인트들일지라도 매니폴드를 보다 저차원 공간으로 맵핑하면 오히려 거리가 멀어질 수 있습니다. 그리고 저차원의 공간상에서 가까운 점끼리는 실제로도 비슷한 특징을 갖습니다.

21페이지
이 책의 저자는 딥러닝이 문제를 풀기 위해 차원 축소를 수행하는 과정은 데이터가 존재하는 고차원상에서 매니폴드를 찾는 과정이라고 얘기합니다. PCA 같이 선형적인 방식에 비해 딥러닝은 비선형적인 방식으로 차원 축소를 수행하며 그 과정에서 해당 문제를 가장 잘 해결하기 위한 매니폴드를 자연스럽게 찾아낸다고 얘기하며 이것이 딥러닝이 성공적으로 동작하는 이유일거라고 예상했습니다.

22페이지
자연어 처리에서 단어를 표현하기 위한 차원 축소 방법들을 소개하기 전에 오토인코더를 먼저 설명하겠습니다. 오토인코더는 원래 입력을 그대로 복원해내는 아키텍처입니다. 고차원의 샘플 벡터를 입력으로 받아 매니폴드를 찾고, 저차원으로 축소하는 인코더를 거쳐 병목구간에서의 숨겨진 벡터로 표현하고 디코더는 저차원의 벡터를 받아 다시 원래 입력 샘플이 존재하던 고차원으로 데이터를 복원합니다. 이 구조의 모델을 훈련할 때는 복원된 데이터와 실제 입력 데이터 사이의 차이를 최소화하도록 손실 함수를 구성합니다. 이때 인코더는 output 인 hidden vector 에 차원은 작아지지만 많은 정보를 우겨 넣어야합니다. 이 과정에서 복원에 필요하지 않은 정보를 버리게 되면서 필요한 정보를 골라내는 학습이 진행됩니다. 학습의 결과로 hidden vector 인 z 를 차원 축소된 dense vector 로 사용할 수 있습니다.

23페이지
4번 흔한 오해1 입니다.

24페이지
흔한 오해는 사전 훈련을 통해 생성된 임베딩 벡터를 텍스트 분류, 언어 모델, 번역 등의 딥러닝 모델의 입력으로 사용한다고 생각하는 것입니다. word2vec 을 통해 얻은 단어 임베딩 벡터가 훌륭하게 단어의 특징을 잘 반영하고 있긴 하지만 텍스트 분류나 언어모델, 번역의 문제를 해결하는 최적의 벡터 임베딩이라고 볼 수 없습니다. 예를들어 감성 분류를 위한 task 에서 '행복'이라는 단어가 매우 중요한 특징이 될 수 있고 기계번역 task 에서 '행복'이라는 그저 일반적인 단어에 지나지 않을 수 있습니다. 이때 task 별로 단어가 가지는 임베딩 벡터가 달라질 수 있습니다. 그래서 문제의 특징을 고려하지 않은 단어 임베딩 벡터는 그다지 좋은 방법이 될 수 없습니다.

25페이지
이래서 word2vec 을 사용하지 않고 문제의 특징에 맞는 단어 임베디 벡터를 구할 수 있도록 여러 딥러닝 프레임워크는 임베딩 레이어 아키텍처를 제공합니다.

26페이지
임베딩 레이어는 입력으로 원핫 벡터가 주어지면 임베딩 레이어 가중치 W 의 특정 행 또는 열을 반환해줍니다. 실제 구현에서는 큰 W 와 원핫 인코딩 벡터(x)를 곱하는 것은 매우 비효율적이므로 테이블에서 검색하는 작업을 수행합니다. 그래서 단어를 나타내는 원핫 벡터를 굳이 넘겨줄 필요 없이 1이 존재하는 단어의 인덱스 정수값만 입력으로 넘겨주면 임베딩 벡터를 구할 수 있습니다.

27페이지
하지만 사전 훈련된 임베딩 벡터를 딥러닝 모델의 직접 넣어 사용하는 경우도 있습니다. 준비된 코퍼스의 양이 너무 적은 경우, 베이스라인 모델을 만드는 용도 또는 고도화된 언어 모델(BERT)을 통해 사전 훈련하여 접근해 볼 수 있습니다.

28페이지
5번 word2vec 입니다.

29페이지
word2vec 은 sparse 한 단어 벡터를 dense vector 로 변환하려는 방법입니다. 주변에 함께 등장하는 단어가 비슷할수록 비슷한 벡터 값을 가질 것이라는 가정을 전제합니다. word2vec 을 통해 얻어진 단어의 대한 vector 가 문장의 문맥에 따라 달라지지는 않습니다. 예를 들어 방탄이라는 단어는 어떤 경우엔 방탄유리와 관련한 단어일 수 있고 예능 쪽에서는 BTS 를 의미할 수 있습니다. 그렇지만 학습하는 corpus 전체를 보고 방탄 이라는 단어는 하나의 vector 로만 학습됩니다. 그리고 context window size 즉 주변에 몇 단어를 볼 것이냐에 따라서 embedding vector 의 성격이 바뀔 수 있습니다.

30페이지
word2vec 을 학습하는 방법은 CBOW 와 Skip-gram 두가지 방법이 있습니다. CBOW 는 주변 단어들이 주어졌을 때 중심단어를 예측하도록 학습하고 skip-gram 은 중심단어가 주어지면 주변 단어를 예측하도록 학습합니다. 주로 skip-gram 이 좀 더 성능이 좋다고 알려져 있습니다.

31페이지
그래서 skip-gram 방식을 설명하도록 하겠습니다. 앞서 설명한 것처럼 중심단어가 주어졌을 때 앞뒤 n 개의 단어를 예측하도록 훈련됩니다. MLE 를 통해 다음 수식의 argmax 내의 수식을 최대로 하는 파라미터 세타를 찾습니다. 이 수식을 풀어보면 중심단어가 주어졌을 때 동시에 나타날 주변 단어들의 확률을 최대로 만드는 세타를 찾는다고 생각할 수 있습니다.

32페이지
이 부분은 좀 헷갈리는데 중심단어(x)가 주어졌을때 W 랑 W' 통과하고 softmax 통과해서 vocab 마다의 주변단어일 확률 벡터 구할 수 있는데 argmax 밑 부분이 정확히 무엇을 최대화하려는건지 이해가 잘 되지 않았습니다.

33페이지
그림으로 보게 되면 중심 단어인 w_t 의 원핫벡터인 x 가 임베딩 레이어에 들어가서 word embedding vector 가 나오고 이게 linear layer 를 통과해 다시 vocab size 만큼의 차원을 갖는 벡터로 바뀌는 과정을 볼 수 있고 softmax 를 거쳐서 중심단어가 들어갔을 떄 주변단어의 확률을 나타내는 벡터가 최종 output 이 됩니다. 결국 중심단어가 input 으로 들어갔을떄 output 으로 주변단어의 확률이 가장크게 나오도록 학습한다고 할 수 있고 중간에 있는 Embedding layer 를 거쳐 나온 벡터가 중심단어의 학습된 embedding vector 가 됩니다.

34페이지
PCA 를 통해 2차원으로 축소해 임베딩 벡터를 시각화해보면 축소된 공간에 단어들이 빽빽하게 분포하는 것을 알 수 있고 비슷한 단어들끼리 가까이 분포되어 있음을 확인할 수 있습니다.

35페이지
6번 GloVe 입니다.

36페이지
GloVe 또한 sparse 한 단어 벡터를 Dense vector 로 변환하려는 방법입니다. word2vec 과는 달리 단어 x 와 코퍼스 내에 함께 출현한 단어들의 출현 빈도를 맞추도록 학습됩니다. word2vec 이 어떤 단어가 주변단어인지를 분류하는 classification 문제였다면 GloVe 는 출현 빈도를 근사하는 회귀 문제입니다. W'Wx 가 중심단어의 대한 output vector 를 반호나하고 단어 x 와 함께 코퍼스에 출현했던 모든 단어의 각 동시 출현 빈도들을 나타낸 벡터인 C_x 에 log 를 씌워 근사하도록 학습합니다. 

37페이지
단어 x 자체의 출현 빈도 또는 사전확률에 따라 MSE 손실 함수의 값이 매우 달라지게 됩니다. 그래서 f(x) 를 곱해주는데 f(x)는 단어의 빈도에 따라 다음과 같이 손실 함수에 가중치를 부여합니다. threshold 보다 빈도가 적으면 빈도에 threshold 를 나눠주고 알파승을 해주고 threshold 보다 빈도가 적지 않다면 원래값을 사용하도록 1을 곱해줍니다. GloVe 논문에서는 실험에 의해 threshold 는 100, alpha 는 4분의 3일때 가장 좋은 결과나 나온다고 언급했습니다.

38페이지
GloVe 의 장점은 한번만 단어별 동시 출현한 단어들의 출현빈도를 조사하면 다시 조사할 필요 없이 반복해서 사용이 가능해 코퍼스 전체를 훌으며 대상 단어와 주변 단어를 가져와 학습하는 과정을 반복하는 skip-gr 과 달리 학습이 빠릅니다. 또한 skip-gram 은 출현빈도 자체가 적은 단어에 대해서는 학습 기회가 적어서 출현 빈도가 적은 단어들은 비교적 부정확한 단어 임베딩 벡터를 학습하게 됩니다. 하지만 GloVe 의 경우에는 출현 빈도가 적더라도 모든 코퍼스에서 동시 출현한 빈도를 사용하므로 출현 빈도가 적은 단어들이 단어 임베딩 벡터를 부정확하게 학습하는 단점이 완벽하진 않지만 어느 정도 보완됩니다.

39페이지
GloVe 가 속도도 빠르고 저런 단점도 보완하지만 skip-gram 도 파라미터 윈도우 크기와 학습률, 학습 반복의 수를 튜닝하면 GloVe 와 큰 성능 차이가 없다고 합니다. 실제 구현할 때는 주어진 상황에 따라 적절한 방식을 선택하는게 좋습니다.

40페이지
제 발표는 여기까지 입니다. 질문 있으시면 질문주세요~