{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"김자딥파_chapter10.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNtWvxdkk15qXLob8l7x3X0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### 10.2.4 파이토치 예제 코드"],"metadata":{"id":"B1MfW6h8tHSi"}},{"cell_type":"code","source":["# language_model.py\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn.utils.rnn import pack_padded_sequence as pack\n","from torch.nn.utils.rnn import pad_packed_sequence as unpack"],"metadata":{"id":"fYJL8AO4tL_F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Encoder(nn.Module):\n","\n","    def __init__(self, word_vec_dim, hidden_size, n_layers=4, dropout_p=.2):\n","        super(Encoder, self).__init__()\n","\n","        # Be aware of value of 'batch_first' parameter.\n","        # Also, its hidden_size is half of original hidden_size, because it is bidirectional.\n","        self.rnn = nn.LSTM(word_vec_dim,\n","                           int(hidden_size / 2),\n","                           num_layers=n_layers,\n","                           dropout=dropout_p,\n","                           bidirectional=True,\n","                           batch_first=True,\n","                           )\n","        \n","    def forward(self, emb):\n","        # |emb| = (batch_size, length, word_vec_dim)\n","\n","        if isinstance(emb, tuple):\n","            x, lengths = emb\n","            x = pack(x, lengths.tolist(), batch_first=True)\n","        else:\n","            x = emb\n","\n","        y, h = self.rnn(x)\n","        # |y| = (batch_size, length, hidden_size)\n","        # |h[0]| = (num_layers * 2, batch_size, hidden_size / 2)\n","\n","        if isinstance(emb, tuple):\n","            y, _ = unpack(y, batch_first=True)\n","\n","        return y, h            \n"],"metadata":{"id":"-o_Z1w8RFcKy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a = [torch.tensor([1, 2, 3]), torch.tensor([3, 4])]\n","b = torch.nn.utils.rnn.pad_sequence(a, batch_first=True)\n","b"],"metadata":{"id":"uZhYT8pAHIfP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647502271134,"user_tz":-540,"elapsed":7,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}},"outputId":"e2fa7cd5-dd99-48e0-ff67-63238a557817"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 2, 3],\n","        [3, 4, 0]])"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["torch.nn.utils.rnn.pad_sequence(a)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4gYdDvrLyZ7F","executionInfo":{"status":"ok","timestamp":1647502274273,"user_tz":-540,"elapsed":4,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}},"outputId":"3ab3ce91-b9c0-4235-e5f2-b280f7eb718a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 3],\n","        [2, 4],\n","        [3, 0]])"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["torch.nn.utils.rnn.pack_padded_sequence(b, batch_first=True, lengths=[3, 2])\n","# output\n","# batch_sizes -> time-step 별 미니배치 개수"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0L76-0GByasq","executionInfo":{"status":"ok","timestamp":1647502630960,"user_tz":-540,"elapsed":282,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}},"outputId":"79a019a4-d0e5-4461-c9c6-5c595cac12b1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PackedSequence(data=tensor([1, 3, 2, 4, 3]), batch_sizes=tensor([2, 2, 1]), sorted_indices=None, unsorted_indices=None)"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["class Generator(nn.Module):\n","\n","    def __init__(self, hidden_size, output_size):\n","        super(Generator, self).__init__()\n","\n","        self.output = nn.Linear(hidden_size, output_size)\n","        self.softmax = nn.LogSoftmax(dim=-1)\n","\n","    def forward(self, x):\n","        # |x| = (batch_size, length, hidden_size)\n","\n","        y = self.softmax(self.output(x))\n","        # |y| = (batch_size, length, output_size)\n","\n","        # Return log-probability instead of just probability.\n","        return y"],"metadata":{"id":"2sX8AFvTzx1r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Default weight for loss equals to 1, but we don't need to get loss for PAD token.\n","# Thus, set a weight for PAD to zero.\n","loss_weight = torch.ones(output_size)\n","loss_weight[data_loader.PAD] = 0."],"metadata":{"id":"m1Vvu-YU1OMD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a = torch.randn(2, 4, 6)\n","a.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xe1wqxd1fpUB","executionInfo":{"status":"ok","timestamp":1647581255855,"user_tz":-540,"elapsed":19,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}},"outputId":"e46aafef-b8ad-4347-8765-d9e462ab5c25"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 4, 6])"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["a.stride()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YTPDjwoEfsZN","executionInfo":{"status":"ok","timestamp":1647581263621,"user_tz":-540,"elapsed":4,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}},"outputId":"84277241-526c-4e4c-eda2-14bd853e5e53"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(24, 6, 1)"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["a = a.transpose(0, 1)\n","a.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q5jUBnCjfvO-","executionInfo":{"status":"ok","timestamp":1647582091406,"user_tz":-540,"elapsed":3,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}},"outputId":"d49e38a5-3934-4a52-e6b9-0b0cf29b6c17"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 2, 6])"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["a.stride()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Nmh9gJ0i5Ta","executionInfo":{"status":"ok","timestamp":1647582096637,"user_tz":-540,"elapsed":4,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}},"outputId":"278539d6-20c8-4060-fb25-fc22f310bb40"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(6, 24, 1)"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["a.is_contiguous()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VNUiQ0aai6i1","executionInfo":{"status":"ok","timestamp":1647582105725,"user_tz":-540,"elapsed":3,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}},"outputId":"cd1c48da-5c0e-477e-cd57-4274a57f64bc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["b = torch.randn(4, 2, 6)\n","b.stride()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DDw1viDsi80Z","executionInfo":{"status":"ok","timestamp":1647582116493,"user_tz":-540,"elapsed":4,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}},"outputId":"5e9c0729-9aff-49e5-a70c-9033824c9890"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(12, 6, 1)"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["b.is_contiguous()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mLlk5li_i_fo","executionInfo":{"status":"ok","timestamp":1647582122099,"user_tz":-540,"elapsed":2,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}},"outputId":"38bc99b7-62f3-4085-d97f-ba2d2c205d13"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["a = a.contiguous()\n","a.stride()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uNfuIYc5jA0t","executionInfo":{"status":"ok","timestamp":1647582487642,"user_tz":-540,"elapsed":5,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}},"outputId":"47e1b72f-18e4-4654-e20d-531c2647f149"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(12, 6, 1)"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["a = a.transpose(0, 1)\n","a = a.view(-1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":184},"id":"TFIi_UdCkaHd","executionInfo":{"status":"error","timestamp":1647582545586,"user_tz":-540,"elapsed":465,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}},"outputId":"f405a522-3e75-41e9-9674-488cf3f9cce8"},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-451d9d588f32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."]}]},{"cell_type":"code","source":["a = a.contiguous()\n","a = a.view(-1)\n","a.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LRNXmJT6koFi","executionInfo":{"status":"ok","timestamp":1647582718518,"user_tz":-540,"elapsed":6,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}},"outputId":"c9d3efd0-a8f5-4975-f02b-15201175f924"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([48])"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["a = torch.randn(4, 2, 6)\n","a = a.transpose(0, 1)\n","a = a.reshape(-1)\n","a.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A2WDBKVhlRAu","executionInfo":{"status":"ok","timestamp":1647582796634,"user_tz":-540,"elapsed":3,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}},"outputId":"c2aea23a-a8dc-4c61-d18c-146af3bcdc4a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([48])"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["### 10.3.2 key-value 함수"],"metadata":{"id":"XHgm_tdnllgC"}},{"cell_type":"code","source":["dic = {'computer': 9, 'dog': 2, 'cat': 3}"],"metadata":{"id":"iXY_RR7knFDv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def key_value_func(query):\n","    weights = []\n","\n","    for key in dic.keys():\n","        weights += [is_same(key, query)]\n","\n","    weight_sum = sum(weights)\n","    for i, w in enumerate(weights):\n","        weights[i] = weights[i] / weight_sum\n","\n","    answer = 0\n","\n","    for weight, value in zip(weights, dic.values()):\n","        answer += weight * value\n","\n","    return answer\n","\n","def is_same(key, query):\n","    if key == query:\n","        return 1.\n","    else:\n","        return .0\n"],"metadata":{"id":"JQ7MAfz8m2W9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 10.3.6 파이토치 예제 코드"],"metadata":{"id":"xMd1AmKCnnHu"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn"],"metadata":{"id":"nA9JahSfn4wP","executionInfo":{"status":"ok","timestamp":1647824449705,"user_tz":-540,"elapsed":6158,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["x = torch.randn(10, 100, 250)\n","y = torch.randn(10, 250, 10)"],"metadata":{"id":"rrgQXSD0_30u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# |x| = (batch_size, n, k)\n","# |y| = (batch_size, k, m)\n","z = torch.bmm(x, y)\n","# |z| = (batch_size, n, m)"],"metadata":{"id":"sRcfV0kM_uyx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["z.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cSlKEnZ___Aj","executionInfo":{"status":"ok","timestamp":1647589720663,"user_tz":-540,"elapsed":410,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}},"outputId":"137a8063-5b22-4b30-f983-96e22f679fcb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([10, 100, 10])"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["class Attention(nn.Module):\n","\n","    def __init__(self, hidden_size):\n","        super(Attention, self).__init__()\n","\n","        self.linear = nn.Linear(hidden_size, hidden_size, bias=False)\n","        self.softma = nn.Softmax(dim=-1)\n","\n","    def forward(self, h_src, h_t_tgt, mask=None):\n","        # |h_src| = (batch_size, length, hidden_size)\n","        # |h_t_tgt| = (batch_size, 1, hidden_size)\n","        # |mask| = (batch_size, length)\n","\n","        query = self.linear(h_t_tgt.squeeze(1)).unsqueeze(-1)\n","        # |query| = (batch_size, hidden_size, 1)\n","\n","        weight = torch.bmm(h_src, query).squeeze(-1)\n","        # |weight| = (batch_size, length)\n","\n","        if mask is not None:\n","            # Set each weight as -inf, if the mask value equals to 1.\n","            # Since the softmax operation makes -inf to 0,\n","            # masked weights would be set to 0 after softmax operation.\n","            # Thus, if the sample is shorter than other samples in mini-batch,\n","            # the weight for empty time-step would be set to 0.\n","            weight.masked_fill_(mask, -float('inf'))  # weight tensor 에서 mask 에서 True 인 부분을 -inf 로 변환\n","        weight = self.softmax(weight)\n","\n","        context_vector = torch.bmm(weight.unsqueeze(1), h_src)\n","        # |context_vector| = (batch_size, 1, hidden_size)\n","\n","        return context_vector"],"metadata":{"id":"ZKWBHTkd__79","executionInfo":{"status":"ok","timestamp":1647824449706,"user_tz":-540,"elapsed":5,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["### 10.4.4 파이토치 예제 코드"],"metadata":{"id":"tU2w_ZS1DeNQ"}},{"cell_type":"code","source":["class Attention(nn.Module):\n","\n","    def __init__(self, hidden_size):\n","        super(Attention, self).__init__()\n","\n","        self.linear = nn.Linear(hidden_size, hidden_size, bias=False)\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, h_src, h_t_tgt, mask=None):\n","        # |h_src| = (batch_size, length, hidden_size)\n","        # |h_t_tgt| = (batch_size, 1, hidden_size)\n","        # |mask| = (batch_size, length)\n","\n","        query = self.linear(h_t_tgt.squeeze(1)).unsqueeze(-1)\n","        # |query| = (batch_size, hidden_size, 1)\n","\n","        weight = torch.bmm(h_src, query).squeeze(-1)\n","        # |weight| = (batch_size, length)\n","        if mask is not None:\n","            # Set each weight as -inf, if the mask value equals to 1.\n","            # Since the softmax operation makes -inf to 0,\n","            # masked weights would be set to 0 after softmax operation.\n","            # Thus, if the sample is shorter than other samples in mini-batch.\n","            # the weight for empty time-step would be set to 0.\n","            weight.masked_fill_(mask, -float('inf'))\n","        weight = self.softmax(weight)\n","\n","        context_vector = torch.bmm(weight.unsqueeze(1), h_src)\n","        # |context_vector| = (batch_size, 1, hidden_size)\n","\n","        return context_vector"],"metadata":{"id":"QqWKBoTQKI1z","executionInfo":{"status":"ok","timestamp":1647824450028,"user_tz":-540,"elapsed":326,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def generate_mask(self, x, length):\n","    mask = []\n","\n","    max_length = max(length)\n","    for l in length:\n","        if max_length - l > 0:\n","            # If the length is shorter than maximum length among samples,\n","            # set last few values to be 1s to remove attention weight.\n","            mask += [torch.cat([x.new_ones(1, l).zero_(),\n","                                x.new_ones(1, (max_length - l))\n","                                ], dim=-1)]\n","        else:\n","            # If the length of the sample equals to maximum length among samples,\n","            # set every value in mask to be 0.\n","            mask += [x.new_ones(1, l).zero_()]\n","\n","    mask = torch.cat(mask, dim=0).byte()\n","\n","    return mask"],"metadata":{"id":"CoDleBLjL4Yt","executionInfo":{"status":"ok","timestamp":1647824450029,"user_tz":-540,"elapsed":5,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class Encoder(nn.Module):\n","\n","    def __init__(self, word_vec_dim, hidden_size, n_layers=4, dropout_p=.2):\n","        super(Encoder, self).__init__()\n","\n","        # Be aware of value of 'batch_first' parameter.\n","        # Also, its hidden_size is half of original hidden_size,\n","        # because it is bidirectional.\n","        self.rnn = nn.LSTM(word_vec_dim,\n","                           int(hidden_size / 2),\n","                           num_layers=n_layers,\n","                           dropout=dropout_p,\n","                           bidirectional=True,\n","                           batch_first=True,\n","                           )\n","        \n","    def forward(self, emb):\n","        # |emb| = (batch_size, length, word_vec_dim)\n","\n","        if isinstance(emb, tuple):\n","            x, length = emb\n","            # pack -> pack_padded_sequence\n","            # input 으로 들어오는 source 문장의 길이 length 를 받아서 가장 긴애로 \n","            # max_length 를 설정하고 나머지 짧은 애들은 pad 추가해서 \n","            # (batch_size, max_length, word_vec_dim) 이 형태로 만듦\n","            # 근데 이게 저런 형태로 시각적으로 저장되지는 않고 저런 형태로 다른 packing 되어 있음\n","            x = pack(x, length.tolist(), batch_first=True)  \n","        else:\n","            x = emb\n","\n","        y, h = self.rnn(x)  # -> output, (h_n, c_n) 형태로 나오는데 output = y (h_n, c_n) = h 로 받았음\n","        # |y| = (batch_size, length, hidden_size)\n","        # |h[0]| = |h_n| = (num_layers * 2, batch_size, hidden_size / 2)\n","\n","        if isinstance(emb, tuple):\n","            y, _ = unpack(y, batch_first=True)\n","\n","        return y, h"],"metadata":{"id":"qo4J0fQSQQa8","executionInfo":{"status":"ok","timestamp":1647824450029,"user_tz":-540,"elapsed":4,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["class Decoder(nn.Module):\n","\n","    def __init__(self, word_vec_dim, hidden_size, n_layers=4, dropout_p=.2):\n","        super(Decoder, self).__init__()\n","\n","        # Be aware of value of 'batch_first' parameter and 'bidirectional' parameter.\n","        self.rnn = nn.LSTM(word_vec_dim + hidden_size,\n","                           hidden_size,\n","                           num_layers=n_layers,\n","                           dropout=dropout_p,\n","                           bidirectional=False,\n","                           batch_first=True,\n","                           )\n","        \n","    def forward(self, emb_t, h_t_1_tilde, h_t_1):\n","        # |emb_t| = (batch_size, 1, word_vec_dim)\n","        # |h_t_1_tilde| = (batch_size, 1, hidden_size)\n","        # |h_t_1[0]| = (n_layers, batch_size, hidden_size)\n","        batch_size = emb_t.size(0)\n","        hidden_size = h_t_1[0].size(-1)\n","\n","        if h_t_1_tilde is None:\n","            # If this is the first time-step,\n","            # tensor.new() -> 같은 device 에 있는 tensor 를 생성 \n","            # torch.Tensor 와는 다른게 device 를 굳이 다시 설정하지 않아도 됨\n","            h_t_1_tilde = emb_t.new(batch_size, 1, hidden_size).zero_()\n","\n","        # Input feeding trick.\n","        x = torch.cat([emb_t, h_t_1_tilde], dim=-1)\n","\n","        # Unlike encoder, decoder must take an input for sequentially.\n","        y, h = self.rnn(x, h_t_1)\n","\n","        return y, h\n","        "],"metadata":{"id":"lOa6LOnuUjU8","executionInfo":{"status":"ok","timestamp":1647824450029,"user_tz":-540,"elapsed":4,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class Generator(nn.Module):\n","\n","    def __init__(self, hidden_size, output_size):\n","        super(Generator, self).__init__()\n","\n","        self.output = nn.Linear(hidden_size, output_size)\n","        self.softmax = nn.LogSoftmax(dim=-1)\n","\n","    def forward(self, x):\n","        # |x| = (batch_size, length, hidden_size)\n","\n","        y = self.softmax(self.output(x))\n","        # |y| = (batch_size, length, output_size)\n","\n","        # Return log-probability instead of just probability.\n","        return y"],"metadata":{"id":"GnRgQiVXXx_J","executionInfo":{"status":"ok","timestamp":1647824694740,"user_tz":-540,"elapsed":275,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class Seq2Seq(nn.Module):\n","\n","    def __init__(self,\n","                 input_size,\n","                 word_vec_dim,\n","                 hidden_size,\n","                 output_size,\n","                 n_layers=4,\n","                 dropout_p=.2,\n","                 ):\n","        self.input_size = input_size\n","        self.word_vec_dim = word_vec_dim\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.n_layers = n_layers\n","        self.dropout_p = dropout_p\n","\n","        super(Seq2Seq, self).__init__()\n","\n","        self.emb_src = nn.Embedding(input_size, word_vec_dim)\n","        self.emb_dec = nn.Embedding(input_size, word_vec_dim)\n","\n","        self.encoder = Encoder(word_vec_dim,\n","                               hidden_size,\n","                               n_layers,\n","                               dropout_p,\n","                               )\n","        self.decoder = Decoder(word_vec_dim,\n","                               hidden_size,\n","                               n_layers,\n","                               dropout_p,\n","                               )\n","        self.attn = Attention(hidden_size)\n","\n","        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n","        self.tanh = nn.Tanh()\n","        self.generator = Generator(hidden_size,\n","                                   output_size,\n","                                   )\n","\n","    def forward(self, src, tgt):\n","        batch_size = tgt.size(0)\n","\n","        mask = None\n","        x_length = None\n","        if isinstance(src, tuple):\n","            x, x_length = src\n","            # based on the length information,\n","            # generate mask to prevent that shorter sample has wasted attention.\n","            mask = self.generate_mask(x, x_length)\n","            # |mask| = (batch_size, length)\n","        else:\n","            x = src\n","\n","        if isinstance(tgt, tuple):\n","            tgt = tgt[0]\n","        \n","        # Get word embedding vectors for every time-step of input sentence.\n","        emb_src = self.emb_src(x)\n","        # |emb_src| = (batch_size, length, word_vec_dim)\n","\n","        # The last hidden state of the encoder would be\n","        # a initial hidden state of decoder.\n","        h_src, h_0_tgt = self.encoder((emb_src, x_length))\n","        # |h_src| = (batch_size, length, hidden_size)\n","        # |h_0_tgt| = (n_layers * 2, batch_size, hidden_size / 2\n","        \n","        # Merge bidirectional to uni-directional\n","        # We need to convert size from (n_layers * 2, batch_size, hidden_size / 2)\n","        # to (n_layers, batch_size, hidden_size).\n","        # Thus, the converting operation will not working with just 'view' method.\n","        h_0_tgt, c_0_tgt = h_0_tgt\n","        h_0_tgt = h_0_tgt.transpose(0, 1).contiguous().view(batch_size,\n","                                                            -1,\n","                                                            self.hidden_size,\n","                                                            ).transpose(0, 1).contiguous()\n","        c_0_tgt = c_0_tgt.transpose(0, 1).contiguous().view(batch_size,\n","                                                            -1,\n","                                                            self.hidden_size,\n","                                                            ).transpose(0, 1).contiguous()\n","        # You can use 'merge_encoder_hiddens' method, instead of using above 3 lines.\n","        # 'merge_encoder_hiddens' method works with non-parallel way.\n","        # h_0_tgt = self.merge_encoder_hiddens(h_0_tgt)\n","        h_0_tgt = (h_0_tgt, c_0_tgt)\n","\n","        emb_tgt = self.emb_dec(tgt)\n","        #|emb_tgt| = (batch_size, length, word_vec_dim)\n","        h_tilde = []\n","        h_t_tilde = None\n","        decoder_hidden = h_0_tgt\n","        # Run deoder until the end of the time-step.\n","        for t in range(tgt.size(1)):\n","            # Teacher Forcing: take each input from training set,\n","            # not from the last time-step's output.\n","            # Because of Teacher Forcing,\n","            # training procedure and inference procedure becomes different.\n","            # Of course, because of sequential running in decoder,\n","            # this cause severe bottle-neck.\n","            emb_t = emb_tgt[:, t, :].unsqueeze(1)\n","            # |emb_t| = (batch_size, 1, word_vec_dim)\n","            # |h_t_tilde| = (batch_size, 1, hidden_size)\n","\n","            decoder_output, decoder_hidden = self.decoder(emb_t,\n","                                                          h_t_tilde,\n","                                                          decoder_hidden)\n","            # |decoder_output| = (batch_size, 1, hidden_size)\n","            # |decoder_hidden| = (n_layers, batch_size, hidden_size)\n","\n","            context_vector = self.attn(h_src, decoder_output, mask)\n","            # |context_vector) = (batch_size, 1, hidden_size)\n","\n","            h_t_tilde = self.tanh(self.concat(torch.cat([decoder_output,\n","                                                         context_vector],\n","                                                        dim=-1)))\n","            # |h_t_tilde| = (batch_size, 1, hidden_size)\n","\n","            h_tilde += [h_t_tilde]\n","\n","        h_tilde = torch.cat(h_tilde, dim=1)\n","        # |h_tilde| = (batch_size, length, hidden_size)\n","\n","        y_hat = self.generator(h_tilde)\n","        # |y_hat| = (batch_size, length, output_size)\n","\n","        return y_hat\n","\n","    def search(self, src, is_greedy=True, max_length=255):\n","        mask, x_length, None, None\n","\n","        if isinstance(src, tuple):\n","            x, x_length = src\n","            mask = self.generate_mask(x, x_length)\n","        else:\n","            x = src\n","        batch_size = x.size(0)\n","\n","        emb_src = self.emb_src(x)\n","        h_src, h_0_tgt = self.encoder((emb_src, x_length))\n","        h_0_tgt, c_0_tgt = h_0_tgt\n","        h_0_tgt = h_0_tgt.transpose(0, 1).contiguous().view(batch_size,\n","                                                            -1, \n","                                                            self.hidden_size,\n","                                                            ).transpose(0, 1).contiguous()\n","        c_0_tgt = c_0_tgt.transpose(0, 1).contiguous().view(batch_size,\n","                                                            -1,\n","                                                            self.hidden_size\n","                                                            ).transpose(0, 1).contiguous()\n","        h_0_tgt = (h_0_tgt, c_0_tgt)\n","\n","        # Fill a vector, which has 'batch_size' dimension, with BOS value.\n","        y = x.new(batch_size, 1).zero_() + data_loader.BOS\n","        is_undone = x.new_ones(batch_size, 1).float()\n","        decoder_hidden = h_0_tgt\n","        h_t_tilde, y_hats, indice = None, [], []\n","\n","        # Repeat a loop while sum of 'is_undone' flag is bigger than 0,\n","        # or current time-step is msaller than maximum length.\n","        while is_undone.sum() > 0 and len(indice) < max_length:\n","            # Unlike training procedure,\n","            # take the last time-step's output during the inference.\n","            emb_t = self.emb_dec(y)\n","            # |emb_t| = (batch_size, 1, word_vec_dim)\n","\n","            decoder_output, decoder_hidden = self.decoder(emb_t,\n","                                                          h_t_tilde,\n","                                                          decoder_hidden)\n","            context_vector = self.attn(h_src, decoder_output, mask)\n","            h_t_tilde = self.tanh(self.concat(torch.cat([decoder_output,\n","                                                         context_vector],\n","                                                        dim=-1)))\n","            y_hat = self.generator(h_t_tilde)\n","            y_hats += [y_hat]\n","\n","            if is_greedy:\n","                y = torch.topk(y_hat, 1, dim=-1)[1].squeeze(-1)\n","            else:\n","                # Take a random sampling based on the multinoulli distribution.\n","                y = torch.multinomial(y_hat.exp().view(batch_size, -1), 1)\n","            # Put PAD if thes ample is done.\n","            y = y.masked_fill_((1. - is_undone).byte(), data_loader.PAD)\n","            is_undone = is_undone * torch.ne(y, data_loader.EOS).float()\n","            # |y| = (batch_size, 1)\n","            # |is_undone| = (batch_size, 1)\n","            indice += [y]\n","        \n","        y_hats = torch.cat(y_hats, dim=1)\n","        indice = torch.cat(indice, im=-1)\n","        # |y_hats| = (batch_size, length, output_size)\n","        # |indice| = (batch_size, length)\n","\n","        return y_hats, indice\n","\n","    def batch_beam_search(self, src, beam_size=5, max_length=255, n_best=1):\n","        mask, x_length = None, None\n","\n","        if isinstance(src, tuple):\n","            x, x_length = src\n","            mask = self.generate_mask(x, x_length)\n","            # |mask| = (batch_size, length)\n","        else:\n","            x = src\n","        batch_size = x.size(0)\n","\n","        emb_src = self.emb_src(x)\n","        h_src, h_0_tgt = self.encoder((emb_src, x_length))\n","        # |h_src| = (batch_size, length, hidden_size)\n","        h_0_tgt, c_0_tgt = h_0_tgt\n","        h_0_tgt = h_0_tgt.transpose(0, 1).contiguous().view(batch_size,\n","                                                            -1,\n","                                                            self.hidden_size,\n","                                                            ).transpose(0, 1).contiguous()\n","        c_0_tgt = c_0_tgt.transpose(0, 1).contiguous().view(batch_size,\n","                                                            -1,\n","                                                            self.hidden_size,\n","                                                            ).transpose(0, 1).contiguous()\n","        # |h_0_tgt| = (n_layers, batch_size, hidden_size)\n","        h_0_tgt = (h_0_tgt, c_0_tgt)\n","\n","        # initialize 'SingleBeamSearchSpace' as many as batch_size\n","        spaces = [SingleBeamSearchSpace((h_0_tgt[0][:, i, :].unsqueeze(1),\n","                                         h_0_tgt[1][:, i, :].unsqueeze(1)\n","                                         ),\n","                                        None,\n","                                        beam_size,\n","                                        max_length=max_length\n","                                        ) for i in range(batch_size)]\n","        done_cnt = [space.is_done() for space in spaces]\n","\n","        length = 0\n","        # Run loop while sum of 'done_cnt' is smaller than batch_size,\n","        # or length is still smaller than max_length.\n","        while sum(done_cnt) < batch_size and length <= max_length:\n","            # current_batch_size = sum(done_cnt) * beam_size\n","\n","            # Initialize fabricated variables.\n","            # As far as batch-beam-search is running,\n","            # temporary batch-size for fabricated mini-batch is\n","            # 'beam_size'-times bigger than original batch_size.\n","            fab_input, fab_hidden, fab_cell, fab_h_t_tilde = [], [], [], []\n","            fab_h_src, fab_mask = [], []\n","\n","            # Build fabricated mini-batch in non-paraellel way.\n","            # This may cause a bottle-neck.\n","            for i, space in enumerate(spaces):\n","                # Batchfy only if the inference for the sample is still not finished.\n","                if space._is_done() == 0:\n","                    y_hat_, (hidden_, cell_), h_t_tilde_ = space.get_batch()\n","                    fab_input += [y_hat_]\n","                    fab_hidden += [hidden_]\n","                    fab_cell += [cell_]\n","                    if h_t_tilde_ is not None:\n","                        fab_h_t_tilde += [h_t_tilde_]\n","                    else:\n","                        fab_h_t_tilde = None\n","\n","                    fab_h_src += [h_src[i, :, :]] * beam_size\n","                    fab_mask += [mask[i, :]] * beam_size\n","\n","            # Now, concatenate list of tensors.\n","            fab_input = torch.cat(fab_input, dim=0)\n","            fab_hidden = torch.cat(fab_hidden, dim=1)\n","            fab_cell = torch.cat(fab_cell, dim=1)\n","            if fab_h_t_tilde is not None:\n","                fab_h_t_tilde = torch.cat(fab_h_t_tilde, dim=0)\n","            fab_h_src = torch.stack(fab_h_src)\n","            fab_mask = torch.stack(fab_mask)\n","            # |fab_input| = (current_batch_size, 1)\n","            # |fab_hidden| = (n_layers, current_batch_size, hidden_size)\n","            # |fab_cell| = (n_layers, current_batch_size, hidden_size)\n","            # |fab_h_t_tilde| = (current_batch_size, 1, hidden_size)\n","            # |fab_h_src| = (current_batch_size, length, hidden_size)\n","            # |fab_mask| = (current_batch_size, length)\n","\n","            emb_t = self.emb_dec(fab_input)\n","            # |emb_t| = (current_batch_size, 1, word_vec_dim)\n","\n","            fab_decoder_output, (fab_hidden, fab_cell) = self.decoder(emb_t,\n","                                                                      fab_h_t_tilde,\n","                                                                      (fab_hidden, fab_cell),\n","                                                                      )\n","            # |fab_decoder_output| = (current_batch_size, 1, hidden_size)\n","            context_vector = self.attn(fab_h_src, fab_decoder_output, fab_mask)\n","            # |contet_vector| = (current_batch_size, 1, hidden_size)\n","            fab_h_t_tilde = self.tanh(self.concat(torch.cat([fab_decoder_output,\n","                                                             context_vector],\n","                                                            dim=-1)))\n","            # |fab_h_t_tilde| = (current_batch_size, 1, hidden_size)\n","            y_hat = self.generator(fab_h_t_tilde)\n","            # |y_hat| = (current_batch_size, 1, output_size)\n","\n","            # separate the result for each sample.\n","            cnt = 0\n","            for space in spaces:\n","                if space.is_done() == 0:\n","                    # Decide a range of each sample.\n","                    from_index = cnt * beam_size\n","                    to_index = from_index + beam_size\n","\n","                    # pick k-best results for each sample.\n","                    space.collect_result(y_hat[from_index:to_index],\n","                                         (fab_hidden[:, from_index:to_index, :],\n","                                          fab_cell[:, from_index:to_index :],\n","                                          ),\n","                                         fab_h_t_tilde[from_index:to_index],\n","                                         )\n","                    cnt += 1\n","\n","            done_cnt = [space.is_done() for space in spaces]\n","            length += 1\n","\n","        # pick n-best hypothesis.\n","        batch_sentences = []\n","        batch_probs = []\n","\n","        # Collect the results.\n","        for i, space in enumerate(spaces):\n","            sentences, probs = space.get_n_best(n_best)\n","\n","            batch_sentences += [sentences]\n","            batch_probs += [probs]\n","\n","        return batch_sentences, batch_probs\n","        \n","            \n","    # 이렇게하면 for 문을 돌기때문에 지양해야함\n","    def merge_encoder_hiddens(self, encoder_hiddens):\n","        new_hiddens = []\n","        new_cells = []\n","\n","        hiddens, cells = encoder_hiddens\n","\n","        # i-th and (i+1)-th layer is opposite direction.\n","        # Also, each direction of layer is half hidden size.\n","        # Therefore, we concatenate both directions to 1 hidden size layer.\n","        for i in range(0, hiddens.size(0), 2):\n","            new_hiddens += [torch.cat([hiddens[i], hiddens[i+1]], dim=-1)]\n","            new_cells += [torch.cat([cells[i], cells[i+1]], dim=-1)]\n","\n","        new_hiddens, new_cells = torch.stack(new_hiddens), torch.stack(new_cells)\n","\n","        return (new_hiddens, new_cells)"],"metadata":{"id":"R_8_4jyKAWov","executionInfo":{"status":"ok","timestamp":1647841121003,"user_tz":-540,"elapsed":1023,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["from operator import itemgetter\n","\n","import torch\n","import torch.nn as nn\n","\n","import data_loader"],"metadata":{"id":"DoZ0tv30u6Yn","executionInfo":{"status":"ok","timestamp":1647836963552,"user_tz":-540,"elapsed":279,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["torch.LongTensor(5).zero_() + data_loader.BOS\n","[torch.FloatTensor([.0] + [-float('inf')] * (5 - 1))]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HcJrVXdXwVHx","executionInfo":{"status":"ok","timestamp":1647838171079,"user_tz":-540,"elapsed":293,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}},"outputId":"cbdb2bce-2ec6-4aaf-e104-c9e00d8614ad"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[tensor([0., -inf, -inf, -inf, -inf])]"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["LENGTH_PENALTY = 1.2\n","MIN_LENGTH = 5\n","\n","\n","class SingleBeamSearchSpace():\n","\n","    def __init__(self,\n","                 hidden,\n","                 h_t_tilde=None,\n","                 beam_size=5,\n","                 max_length=255,\n","                 ):\n","        self.beam_size = beam_size\n","        self.max_length = max_length\n","\n","        super(SingleBeamSearchSpace, self).__init__()\n","\n","        # To put data to same device.\n","        self.device = hidden[0].device\n","        # Inferred word index for each time-step.\n","        # For now, initialized with initial time-step.\n","        self.word_indice = [torch.LongTensor(beam_size).zero_().to(self.device) + data_loader.BOS]\n","        # Index origin of current beam.\n","        self.prev_beam_indice = [torch.LongTensor(beam_size).zero_().to(self.device) - 1]\n","        # Cumulative log-probability for each beam.\n","        self.cumulative_probs = [torch.FloatTensor([.0] + [-float('inf')] * (beam_size - 1)).to(self.device)]\n","        # 1 if it is done else 0\n","        self.masks = [torch.ByteTensor(beam_size).zero_().to(self.device)]\n","\n","        # We don't need to remember every time-step of hidden states:\n","        # prev_hidden, prev_cell, prev_h_t_tilde\n","        # What we need is remember just last one.\n","\n","        # |hidden[0]| = (n_layers, 1, hidden_size)\n","        self.prev_hidden = torch.cat([hidden[0]] * beam_size, dim=1)\n","        self.prev_cell = torch.cat([hidden[1]] * beam_size, dim=1)\n","        # |prev_hidden| = (n_layers, beam_size, hidden_size)\n","        # |prev_cell| = (n_layers, beam_size, hidden_size)\n","\n","        # |h_t_tilde| = (batch_size = 1, 1, hidden_size)\n","        self.prev_h_t_tilde = torch.cat([h_t_tilde] * beam_size,\n","                                        dim=0\n","                                        ) if h_t_tilde is not None else None\n","        # |prev_h_t_tilde| = (beam_size, 1, hidden_size)\n","\n","        self.current_time_step = 0\n","        self.done_cnt = 0\n","\n","    def get_length_penalty(self,\n","                           length,\n","                           alpha=LENGTH_PENALTY,\n","                           min_length=MIN_LENGTH,\n","                           ):\n","        # Calculate length-penalty,\n","        # because shorter sentence usually have bigger probability.\n","        # Thus, we need to put penalty for shorter one.\n","        p = (1 + length) ** alpha / (1 + min_length) ** alpha\n","\n","        return p\n","\n","    def is_done(self):\n","        # Return 1, if we had EOS more than 'beam_size'-times.\n","        if self.don_cnt >= self.beam_size:\n","            return 1\n","        return 0\n","\n","    def get_batch(self):\n","        y_hat = self.word_indice[-1].unsqueeze(-1)\n","        hidden = (self.prev_hidden, self.prev_cell)\n","        h_t_tilde = self.prev_h_t_tilde\n","\n","        # |y_hat| = (beam_size, 1)\n","        # |hidden| = (n_layers, beam_size, hidden_size)\n","        # |h_t_tilde| = (beam_size, 1, hidden_size) or None\n","        return y_hat, hidden, h_t_tilde\n","\n","    def collect_result(self, y_hat, hidden, h_t_tilde):\n","        # |y_hat| = (beam_size, 1, output_size)\n","        # |hidden| = (n_layers, beam_size, hidden_size)\n","        # |h_t_tilde| = (beam_size, 1, hidden_size)\n","        output_size = y_hat.size(-1)\n","\n","        self.current_time_step += 1\n","\n","        # Calculate cummulative log-probability.\n","        # First, fill -inf value to last cumulative probability,\n","        # if the beam is already finished.\n","        # Second, expand -inf filled cumulative probability to fit to 'y_hat'.\n","        # (beam_size) --> (beam_size, 1, 1) --> (beam_size, 1, output_size)\n","        # Third, add expanded cumulative probability to 'y_hat'\n","        cumulative_prob = y_hat + self.cumulative_probs[-1].masked_fill_(self.masks[-1],\n","                                                                         -float('inf')).view(-1, 1, 1).expand(self.beam_size, 1, output_size)\n","        # Now, we have new top log-probability and its index.\n","        # We picked top index as many as 'beam_size'.\n","        # Be aware that we picked top-k from whole batch through 'view(-1)'.\n","        top_log_prob, top_indice = torch.topk(cumulative_prob.view(-1),\n","                                              self.beam_size,\n","                                              dim=-1)\n","        # |top_log_prob| = (beam_size)\n","        # |top_indice| = (batch_size)\n","\n","        # Because we picked from whole batch,\n","        # original word index should be calculated again.\n","        self.word_indice += [top_indice.fmod(output_size)]\n","        # Also, we can get an index of beam,\n","        # which has top-k log-probability search result.\n","        self.prev_beam_indice += [top_indice.div(output_size).long()]\n","\n","        # Add results to history boards.\n","        self.cumulative_probs += [top_log_prob]\n","        self.masks += [torch.eq(self.word_indice[-1],\n","                                data_loader.EOS)\n","        ]  # Set finish mask if we got EOS.\n","        # Calculate a number of finished beams.\n","        self.done_cnt += self.masks[-1].float().sum()\n","\n","        # Set hidden states for next time-step, usng 'index_select' method.\n","        self.prev_hidden = torch.index_select(hidden[0],\n","                                              dim=1,\n","                                              index=self.prev_beam_indice[-1]\n","                                              ).contiguous()\n","        self.prev_cell = torch.index_select(hidden[1],\n","                                            dim=1,\n","                                            index=self.prev_beam_indice[-1]\n","                                            ).contiguous()\n","        self.prev_h_t_tilde = torch.index_select(h_t_tilde,\n","                                                 dim=0,\n","                                                 index=self.prev_beam_indice[-1]\n","                                                 ).contiguous()\n","\n","    def get_n_best(self, n=1):\n","        sentences, probs, founds = [], [], []\n","\n","        for t in range(len(self.word_indice)):  # for each time-step,\n","            for b in range(self.beam_size):  # for each beam,\n","                if self.masks[t][b] == 1: # if we had EOS on this time-step and beam,\n","                    # Take a record of penaltified log-probability.\n","                    probs += [self.cumulative_probs[t][b] / self.get_length_penalty(t)]\n","                    founds += [(t, b)]\n","\n","        # Also, collect log-probability from last time-step, for the case of EOS is not shown.\n","        for b in range(self.beam_size):\n","            if self.cumulative_probs[-1][b] != -float('inf'):\n","                if not (len(self.cumulative_probs) - 1, b) in founds:\n","                    probs += [self.cumulative_probs[-1][b]]\n","                    founds += [(t, b)]\n","\n","        # Sort and take n-best.\n","        sorted_founds_with_probs = sorted(zip(founds, probs),\n","                                          key=itemgetter(1),\n","                                          reverse=True,\n","                                          )[:n]\n","\n","        probs = []\n","\n","        for (end_index, b), prob in sorted_founds_with_probs:\n","            sentence = []\n","\n","            # Trace from the end.\n","            for t in range(end_index, 0, -1):\n","                sentence = [self.word_indice[t][b]] + sentence\n","                b = self.prev_beam_indice[t][b]\n","\n","            sentences += [sentence]\n","            probs += [prob]\n","\n","        return sentences, probs"],"metadata":{"id":"7F7lHFHDvKA0"},"execution_count":null,"outputs":[]}]}