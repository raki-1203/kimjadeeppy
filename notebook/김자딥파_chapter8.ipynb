{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"김자딥파_chapter8.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMYKjd0fx6NC+oNQNsmX7te"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### 8.4.2 파이토치 예제 구현하기"],"metadata":{"id":"J9muzu3pBVdx"}},{"cell_type":"code","source":["import torch.nn as nn"],"metadata":{"id":"443qfhXSBbBX","executionInfo":{"status":"ok","timestamp":1647237789758,"user_tz":-540,"elapsed":6486,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["class RNNClassifier(nn.Module):\n","\n","    def __init__(self,\n","                 input_size,\n","                 word_vec_dim,\n","                 hidden_size,\n","                 n_classes,\n","                 n_layers=4,\n","                 dropout_p=.3,\n","                 ):\n","        self.input_size = input_size # vocabulary_size\n","        self.word_vec_dim = word_vec_dim\n","        self.hidden_size = hidden_size\n","        self.n_classes = n_classes\n","        self.n_layers = n_layers\n","        self.dropout_p = dropout_p\n","\n","        super().__init__()\n","\n","        self.emb = nn.Embedding(input_size, word_vec_dim)\n","        self.rnn = nn.LSTM(input_size=word_vec_dim,\n","                           hidden_size=hidden_size,\n","                           num_layers=n_layers,\n","                           dropout=dropout_p,\n","                           batch_first=True,\n","                           bidirectional=True,\n","                           )\n","        \n","        self.generator = nn.Linear(hidden_size * 2, n_classes)\n","        # We use LogSoftmax + NLLLoss instead of Softmax + CrossEntropy\n","        self.activation = nn.LogSoftmax(dim=-1)\n","\n","    def forward(self, x):\n","        # |x| = (batch_size, length)\n","        x = self.emb(x)\n","        # |x| = (batch_size, length, word_vec_dim)\n","        x, _ = self.rnn(x)\n","        # |x| = (batch_size, length, hidden_size * 2)\n","        y = self.activation(self.generator(x[:, -1]))\n","        # |y| = (batch_size, n_classes)\n","\n","        return y\n"],"metadata":{"id":"rOL8xg7kBdFK","executionInfo":{"status":"ok","timestamp":1647240806973,"user_tz":-540,"elapsed":346,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn"],"metadata":{"id":"83mbzyRWCJgj","executionInfo":{"status":"ok","timestamp":1647242452301,"user_tz":-540,"elapsed":337,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class CNNClassifier(nn.Module):\n","\n","    def __init__(self,\n","                 input_size,\n","                 word_vec_dim,\n","                 n_classes,\n","                 dropout_p=.5,\n","                 window_sizes=[3, 4, 5],\n","                 n_filters=[100, 100, 100],\n","                 ):\n","        self.input_size = input_size\n","        self.word_vec_dim = word_vec_dim\n","        self.n_classes = n_classes\n","        self.dropout_p = dropout_p\n","        # window_size means that how many words a pattern covers.\n","        self.window_sizes = window_sizes\n","        # n_filters means that how many patterns to cover.\n","        self.n_filters = n_filters\n","\n","        super().__init__()\n","\n","        self.emb = nn.Embedding(input_size, word_vec_dim)\n","        # Since number of convolution layers would be vary depend on len(window_sizes),\n","        # we use 'setattr' and 'getattr' methods to add layers to nn.Module object.\n","        for window_size, n_filter in zip(window_sizes, n_filters):\n","            cnn = nn.Conv2d(in_channels=1,\n","                            out_channels=n_filters,\n","                            kernel_size=(window_size, word_vec_dim),\n","                            )\n","            setattr(self, 'cnn-%d-%d' % (window_size, n_filter), cnn)\n","        # Because below layers are just operations,\n","        # (it does not have learnable parameters)\n","        # we just declare once.\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout_p)\n","        # An input of generator layer is max values from each filter.\n","        self.generator = nn.Linear(sum(n_filters), n_classes)\n","        # We use LogSoftmax + NLLLoss instead of Softmax + CrossEntropy\n","        self.activation = nn.LogSoftmax(dim=-1)\n","\n","    def forward(self, x):\n","        # |x| = (batch_size, length)\n","        x = self.emb(x)\n","        # |x| = (batch_size, length, word_vec_dim)\n","        min_length = max(self.window_sizes)\n","        if min_length > x.size(1):\n","            # Because some input does not long enough for maximum length of window size,\n","            # we add zero tensor for padding.\n","            pad = x.new(x.size(0), min_length - x.size(1), self.word_vec_dim).zero_()\n","            # |pad| = (batch_size, min_length - length, word_vec_dim)\n","            x = torch.cat([x, pad], dim=1)\n","            # |x| = (batch_size, min_length, word_vec_dim)\n","\n","        # In ordinary case of vision task, you may have 3 chanels on tensor,\n","        # but in this case, you would have just 1 channel,\n","        # which is added by 'unsqueeze' method in below:\n","        x = x.unsqueeze(1)\n","        # |x| = (batch_size, 1, length, word_vec_dim)\n","\n","        cnn_outs = []\n","        for window_size, n_filter in zip(self.window_sizes, self.n_filters):\n","            cnn = getattr(self, 'cnn-%d-%d' % (window_size, n_filter))\n","            cnn_out = self.dropout(self.relu(cnn(x)))\n","            # |cnn_out| = (batch_size, n_filter, length - window_size + 1, 1)\n","\n","            # In case of max pooling, we does not know the pooling size,\n","            # because it depends on the length of the sentene.\n","            # Therefore, we use instant function using 'nn.functional' package.\n","            # This is the beauty of PyTorch. :)\n","            cnn_out = nn.functional.max_pool1d(input=cnn_out.squeeze(-1),\n","                                               kernel_size=cnn_out.size(-2),\n","                                               ).squeeze(-1)\n","            # |cnn_out| = (batch_size, n_filters)\n","            cnn_outs += [cnn_out]\n","\n","        # Merge output tensors from each convolution layer.\n","        cnn_outs = torch.cat(cnn_outs, dim=-1)\n","        # |cnn_outs| = (batch_size, sum(n_filters))\n","        y = self.activation(self.generator(cnn_outs))\n","        # |y| = (batch_size, n_classes)\n","\n","        return y\n","\n","            "],"metadata":{"id":"jYq4x1NETRo4","executionInfo":{"status":"ok","timestamp":1647243850439,"user_tz":-540,"elapsed":343,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"RC6Q-fVzVR72"},"execution_count":null,"outputs":[]}]}