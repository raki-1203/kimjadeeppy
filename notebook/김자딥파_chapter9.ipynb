{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"김자딥파_chapter9.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOrZOp6hsNZABnmv1+TggYn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### 9.5.4 파이토치 예제"],"metadata":{"id":"B1MfW6h8tHSi"}},{"cell_type":"code","source":["# language_model.py\n","\n","import torch\n","import torch.nn as nn"],"metadata":{"id":"fYJL8AO4tL_F","executionInfo":{"status":"ok","timestamp":1647306188099,"user_tz":-540,"elapsed":300,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class data_loader:\n","    PAD, BOS, EOS = 1, 2, 3\n"],"metadata":{"id":"-o_Z1w8RFcKy","executionInfo":{"status":"ok","timestamp":1647306182697,"user_tz":-540,"elapsed":3,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class LanguageModel(nn.Module):\n","\n","    def __init__(self,\n","                 vocab_size,\n","                 word_vec_dim=512,\n","                 hidden_size=512,\n","                 n_layers=4,\n","                 dropout_p=.2,\n","                 max_length=255,\n","                 ):\n","        self.vocab_size = vocab_size\n","        self.word_vec_dim = word_vec_dim\n","        self.hidden_size = hidden_size\n","        self.n_layers = n_layers\n","        self.dropout_p = dropout_p\n","        self.max_length = max_length\n","\n","        super(LanguageModel, self).__init__()\n","\n","        self.emb = nn.Embedding(vocab_size,\n","                                word_vec_dim,\n","                                padding_idx=data_loader.PAD,\n","                                )\n","        self.rnn = nn.LSTM(word_vec_dim,\n","                           hidden_size,\n","                           n_layers,\n","                           batch_first=True,\n","                           dropout=dropout_p,\n","                           )\n","        self.out = nn.Linear(hidden_size, vocab_size, bias=True)\n","        self.log_softmax = nn.LogSOftmax(dim=2)\n","\n","    def forward(self, x):\n","        # |x| = (batch_size, length)\n","        x = self.emb(x)\n","        # |x| = (batch_size, length, word_vec_dim)\n","        x, (h, c) = self.rnn(x)\n","        # |x| = (batch_size, length, hidden_size)\n","        x = self.out(x)\n","        # |x| = (batch_size, lenfth, vocab_size)\n","        y_hat = self.log_softmax(x)\n","\n","        return y_hat\n","\n","    def search(self, batch_size=64, max_length=255):\n","        x = torch.LongTensor(batch_size, 1).to(next(self.parameters()).device).zero_() + data_loader.BOS\n","        # |x| = (batch_size, 1)\n","        is_undone = x.new_ones(batch_size, 1).float()\n","\n","        y_hats, indice = [], []\n","        h, c = None, None\n","        while is_undone.sum() > 0 and len(indice) < max_length:\n","            x = self.emb(x)\n","            # |emb_t| = (batch_size, 1, word_vec_dim)\n","\n","            x, (h, c) = self.rnn(x, (h, c)) if h is not None and c is not None else self.rnn(x)\n","            # |x| = (batch_size, 1, hidden_size)\n","            y_hat = self.log_softmax(x)\n","            # |y_hat| = (batch_size, 1, output_size)\n","            y_hats += [y_hat]\n","\n","            # y = torch.topk(y_hat, 1, dim=-1)[1].squeeze(-1)\n","            y = torch.multinomial(y_hat.exp().view(batch_size, -1), 1)\n","            y = y.masked_fill((1. - is_undone).byte(), data_loader.PAD)\n","            is_undone = is_undone * torch.ne(y, data_loader.EOS).float()\n","            # |y| = (batch_size, 1)\n","            # |is_undone| = (batch_size, 1)\n","            indice += [y]\n","\n","            x = y\n","\n","        y_hats = torch.cat(y_hats, dim=1)\n","        indice = torch.cat(indice, dim=-1)\n","        # |y_hat| = (batch_size, length, output_size)\n","        # |indice| = (batch_size, length)\n","\n","        return y_hats, indice"],"metadata":{"id":"gufjeF1ZtQl8","executionInfo":{"status":"ok","timestamp":1647306190829,"user_tz":-540,"elapsed":394,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["x = torch.LongTensor(10, 1).to('cpu').zero_() + data_loader.BOS\n","is_undone = x.new_ones(10, 1).float()\n","is_undone"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qnDQL8RZva_j","executionInfo":{"status":"ok","timestamp":1647306234778,"user_tz":-540,"elapsed":263,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}},"outputId":"bc2c39af-f5b4-4424-9e08-58a7b8c4c71c"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.]])"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["torch.multinomial(torch.tensor([0.4, 0.2, 0.1, 0.5, 0.123, 0.123, 0.12, 0.156, 0.68, 1.95]).exp().view(10, -1), 1).masked_fill_((1. - is_undone).byte(), data_loader.PAD)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-iMNL1Y4FB3h","executionInfo":{"status":"ok","timestamp":1647306455019,"user_tz":-540,"elapsed":4,"user":{"displayName":"손희락","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3Gnwu5KqtJnsIScT6lnXAj-wVzbCkZkHGGPK9gA=s64","userId":"06901493583563790110"}},"outputId":"c38c2006-de9e-47b9-8ec4-fc579e2dbbb8"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:1273.)\n","  \"\"\"Entry point for launching an IPython kernel.\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0]])"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":[""],"metadata":{"id":"uZhYT8pAHIfP"},"execution_count":null,"outputs":[]}]}